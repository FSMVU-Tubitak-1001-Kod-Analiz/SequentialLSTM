{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies.\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# hyper-parameters. (affect GPU memory size)\n",
    "_DiffEmbedDim_  = 128       # 128\n",
    "_DiffMaxLen_    = 600       # 200(0.7), 314(0.8), 609(0.9), 1100(0.95), 2200(0.98), 3289(0.99), 5000(0.995), 10000(0.9997)\n",
    "_DRnnHidSiz_    = 16        # 16\n",
    "_MRnnHidSiz_    = 32        # 16\n",
    "_TwinEmbedDim_  = 128       # 128\n",
    "_TwinMaxLen_    = 800       # 224(0.8), 425(0.9), 755(0.95), 1448(0.98), 2270(0.99)\n",
    "_TRnnHidSiz_    = 32        # 16\n",
    "# hyper-parameters. (affect training speed)\n",
    "_DRnnBatchSz_   = 128       # 128\n",
    "_DRnnLearnRt_   = 0.0001    # 0.0001\n",
    "_MRnnBatchSz_   = 128       # 128\n",
    "_MRnnLearnRt_   = 0.0001    # 0.0001\n",
    "_PRnnBatchSz_   = 256       # 256\n",
    "_PRnnLearnRt_   = 0.0005    # 0.0005\n",
    "_TRnnBatchSz_   = 256       # 256\n",
    "_TRnnLearnRt_   = 0.0005    # 0.0005\n",
    "# hyper-parameters. (trivial network parameters, unnecessary to modify)\n",
    "_DiffExtraDim_  = 2         # 2\n",
    "_TwinExtraDim_  = 1         # 1\n",
    "_DRnnHidLay_    = 1         # 1\n",
    "_MRnnHidLay_    = 1         # 1\n",
    "_TRnnHidLay_    = 1         # 1\n",
    "# hyper-parameters. (epoch related parameters, unnecessary to modify)\n",
    "_DRnnMaxEpoch_  = 1000      # 1000\n",
    "_DRnnPerEpoch_  = 1         # 1\n",
    "_DRnnJudEpoch_  = 10        # 10\n",
    "_MRnnMaxEpoch_  = 1000      # 1000\n",
    "_MRnnPerEpoch_  = 1         # 1\n",
    "_MRnnJudEpoch_  = 10        # 10\n",
    "_PRnnMaxEpoch_  = 1000      # 1000\n",
    "_PRnnPerEpoch_  = 1         # 1\n",
    "_PRnnJudEpoch_  = 10        # 10\n",
    "_TRnnMaxEpoch_  = 1000      # 1000\n",
    "_TRnnPerEpoch_  = 1         # 1\n",
    "_TRnnJudEpoch_  = 10        # 10\n",
    "# hyper-parameters. (flow control)\n",
    "_DEBUG_ = 0 #  0 : release\n",
    "            #  1 : debug\n",
    "_LOCK_  = 0 #  0 : unlocked - create random split sets.\n",
    "            #  1 : locked   - use the saved split sets.\n",
    "_MODEL_ = 0 #  0 : unlocked - train a new model.\n",
    "            #  1 : locked   - load the saved model.\n",
    "_DTYP_  = 1 #  0 : maintain both diff code and context code.\n",
    "            #  1 : only maintain diff code.\n",
    "_CTYP_  = 0 #  0 : maintain both the code and comments.\n",
    "            #  1 : only maintain code and delete comments.\n",
    "_NIND_ =  1 # -1 : not abstract tokens. (and will disable _NLIT_)\n",
    "            #  0 : abstract identifiers with VAR/FUNC.\n",
    "            #  1 : abstract identifiers with VARn/FUNCn.\n",
    "_NLIT_  = 1 #  0 : abstract literals with LITERAL.\n",
    "            #  1 : abstract literals with LITERAL/n.\n",
    "_TWIN_  = 1 #  0 : only twin neural network.\n",
    "            #  1 : twins + msg neural network.\n",
    "\n",
    "# print setting.\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# Read the modified file\n",
    "df = pd.read_csv(\"./data.csv\")\n",
    "\n",
    "# Extract the \"smellKey\" column values\n",
    "smellKey = df['label'].values\n",
    "\n",
    "# Print the contents of smell key list\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load the contents of function_smell_embeddings.pt\n",
    "old_embeddings = df['num1']\n",
    "fixed_embeddings = df['num2']\n",
    "\n",
    "# Print the embeddings\n",
    "fixed_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\huday\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load('data.npy',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwinRNNTrain(dataTrain, labelTrain, dataTest, labelTest, preWTwin=twinPreWeights, preWMsg=msgPreWeights,\n",
    "                             batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine old and fixed embeddings\n",
    "twinData = np.concatenate((old_embeddings, fixed_embeddings))\n",
    "label = smellKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinRNN(nn.Module):\n",
    "    '''\n",
    "    TwinRNN : convert a patch data into a predicted label.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, preWTwin, hidSizTwin=32, hidSizMsg=32, hidLayTwin=1, hidLayMsg=1):\n",
    "        '''\n",
    "        define each layer in the network model.\n",
    "        :param preWTwin: tensor pre-trained weights for embedding layer for twin.\n",
    "        :param preWMsg: tensor pre-trained weights for embedding layer for msg.\n",
    "        :param hidSizTwin: node number in the hidden layer for twin.\n",
    "        :param hidSizMsg: node number in the hidden layer for msg.\n",
    "        :param hidLayTwin: number of hidden layer for twin.\n",
    "        :param hidLayMsg: number of hidden layer for msg.\n",
    "        '''\n",
    "\n",
    "        super(TwinRNN, self).__init__()\n",
    "        # parameters.\n",
    "        class_num = 2 # yes no\n",
    "        # twin.\n",
    "        vSizTwin, emDimTwin = preWTwin.size()\n",
    "        # Embedding Layer for twin.\n",
    "        self.embedTwin = nn.Embedding(num_embeddings=vSizTwin, embedding_dim=emDimTwin)\n",
    "        self.embedTwin.load_state_dict({'weight': preWTwin})\n",
    "        self.embedTwin.weight.requires_grad = True\n",
    "        # LSTM Layer for twin.\n",
    "        if _DEBUG_: print(_TwinExtraDim_)\n",
    "        self.lstmTwin = nn.LSTM(input_size=emDimTwin+_TwinExtraDim_, hidden_size=hidSizTwin, num_layers=hidLayTwin, bidirectional=True)\n",
    "\n",
    "        # Fully-Connected Layer.\n",
    "        self.fc1 = nn.Linear(hidSizTwin * hidLayTwin * 4, hidSizTwin * hidLayTwin * 2)\n",
    "        self.fc2 = nn.Linear(hidSizTwin * hidLayTwin * 2, class_num)\n",
    "        # Softmax non-linearity.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        convert inputs to predictions.\n",
    "        :param x: input tensor. dimension: batch_size * twin_length * feature_dim.\n",
    "        :return: self.softmax(final_out) - predictions.\n",
    "        [[0.3, 0.7], [0.2, 0.8], ...]\n",
    "        '''\n",
    "\n",
    "        # twin 1.\n",
    "        xTwin = x[:, :_TwinMaxLen_, :6]\n",
    "        # xTwin         batch_size * twin_length * feature_dim\n",
    "        #print(xTwin.size())\n",
    "        embedsTwin = self.embedTwin(xTwin[:, :, 0])\n",
    "        # embedsTwin    batch_size * twin_length * embed_dim_twin\n",
    "        features = xTwin[:, :, 1:]\n",
    "        # features      batch_size * twin_length * _TwinExtraDim_\n",
    "        inputsTwin = torch.cat((embedsTwin.float(), features.float()), 2) # 2 is the dimension to concatenate\n",
    "        print(inputsTwin.size())\n",
    "        # inputsTwin    batch_size * twin_length * (embed_dim_twin + _TwinExtraDim_)\n",
    "        inputsTwin = inputsTwin.permute(1, 0, 2)\n",
    "        # inputsTwin    twin_length * batch_size * (embed_dim_twin + _TwinExtraDim_)\n",
    "        lstm_out, (h_n, c_n) = self.lstmTwin(inputsTwin)\n",
    "        # lstm_out      twin_length * batch_size * (hidden_size * direction_num)\n",
    "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
    "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
    "        featMapTwin1 = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
    "        # featMapTwin1   batch_size * (hidden_size * num_layers * direction_num)\n",
    "        #print(featMapTwin1)\n",
    "    # twin 2.\n",
    "        xTwin = x[:, :_TwinMaxLen_, 6:-1]\n",
    "        # xTwin         batch_size * twin_length * feature_dim\n",
    "        #print(xTwin.size())\n",
    "        embedsTwin = self.embedTwin(xTwin[:, :, 0])\n",
    "        # embedsTwin    batch_size * twin_length * embed_dim_twin\n",
    "        features = xTwin[:, :, 1:]\n",
    "        # features      batch_size * twin_length * _TwinExtraDim_\n",
    "        inputsTwin = torch.cat((embedsTwin.float(), features.float()), 2)\n",
    "        #print(inputsTwin.size())\n",
    "        # inputsTwin    batch_size * twin_length * (embed_dim_twin + _TwinExtraDim_)\n",
    "        inputsTwin = inputsTwin.permute(1, 0, 2)\n",
    "        # inputsTwin    twin_length * batch_size * (embed_dim_twin + _TwinExtraDim_)\n",
    "        lstm_out, (h_n, c_n) = self.lstmTwin(inputsTwin)\n",
    "        # lstm_out      twin_length * batch_size * (hidden_size * direction_num)\n",
    "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
    "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
    "        featMapTwin2 = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
    "        # featMapTwin2   batch_size * (hidden_size * num_layers * direction_num)\n",
    "        #print(featMapTwin2)\n",
    "\n",
    "        # common.\n",
    "        # combine twins.\n",
    "        featMap = torch.cat((featMapTwin1, featMapTwin2), dim=1)\n",
    "        # fc layers.\n",
    "        featMap = self.fc1(featMap)\n",
    "        if (0 == _TWIN_): # (only twins).\n",
    "            final_out = self.fc2(featMap)\n",
    "        #print(final_out.size())\n",
    "        return self.softmax(final_out)      # batch_size * class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (1 == _TWIN_): # (twins + msg).\n",
    "    # combine twins + msg.\n",
    "    featMap = torch.cat((featMap, featMapMsg), dim=1)\n",
    "    # fc 2 layers.\n",
    "    featMap = self.fc3(featMap)\n",
    "    final_out = self.fc4(featMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare your data\n",
    "twinData = np.concatenate((old_embeddings, fixed_embeddings), axis = 0)\n",
    "label = smellKey\n",
    "\n",
    "print(twinData.shape)\n",
    "# Assuming you have your data prepared as tensors X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwinRNNTrain(dTrain, lTrain, dValid, lValid, preWTwin, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n",
    "    '''\n",
    "    Train the TwinRNN model.\n",
    "    :param dTrain: training data. [[n, ...], ...]\n",
    "    :param lTrain: training label. [[n, ...], ...]\n",
    "    :param dValid: validation data. [[n, ...], ...]\n",
    "    :param lValid: validation label. [[n, ...], ...]\n",
    "    :param preWDiff: pre-trained weights for diff embedding layer.\n",
    "    :param batchsize: number of samples in a batch.\n",
    "    :param learnRate: learning rate.\n",
    "    :param dTest: test data. [[n, ...], ...]\n",
    "    :param lTest: test label. [[n, ...], ...]\n",
    "    :return: model - the TwinRNN model.\n",
    "    '''\n",
    "\n",
    "    # get the mark of the test dataset.\n",
    "    if dTest is None: dTest = []\n",
    "    if lTest is None: lTest = []\n",
    "    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n",
    "\n",
    "    # tensor data processing.\n",
    "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
    "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
    "    xValid = torch.from_numpy(dValid).long().cuda()\n",
    "    yValid = torch.from_numpy(lValid).long().cuda()\n",
    "    if (markTest):\n",
    "        xTest = torch.from_numpy(dTest).long().cuda()\n",
    "        yTest = torch.from_numpy(lTest).long().cuda()\n",
    "\n",
    "    # batch size processing.\n",
    "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
    "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
    "    valid = torchdata.TensorDataset(xValid, yValid)\n",
    "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
    "    if (markTest):\n",
    "        test = torchdata.TensorDataset(xTest, yTest)\n",
    "        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
    "\"\"\"\n",
    "    # get training weights.\n",
    "    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n",
    "    weights = []\n",
    "    for lb in range(2):\n",
    "        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n",
    "    lbWeights = torch.FloatTensor(weights).cuda()\n",
    "\"\"\"\n",
    "    # build the model of recurrent neural network.\n",
    "    preWTwin = torch.from_numpy(preWTwin)\n",
    "    model = TwinRNN(preWTwin, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print('[INFO] <TwinRNNTrain> ModelType: TwinRNN.')\n",
    "    print('[INFO] <TwinRNNTrain> Code Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_TwinEmbedDim_, _TwinMaxLen_, _TRnnHidSiz_, _TRnnHidLay_))\n",
    "    print('[INFO] <TwinRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d, JudEpoch: %d.' % (batchsize, learnRate, _TRnnMaxEpoch_, _TRnnPerEpoch_, _TRnnJudEpoch_))\n",
    "    # optimizing with stochastic gradient descent.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
    "    # seting loss function as mean squared error.\n",
    "    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n",
    "    # memory\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # run on each epoch.\n",
    "    accList = [0]\n",
    "    for epoch in range(_TRnnMaxEpoch_):\n",
    "        # training phase.\n",
    "        model.train()\n",
    "        lossTrain = 0\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for iter, (data, label) in enumerate(trainloader):\n",
    "            # data conversion.\n",
    "            data = data.to(device)\n",
    "            label = label.contiguous().view(-1)\n",
    "            label = label.to(device)\n",
    "            # back propagation.\n",
    "            optimizer.zero_grad()  # set the gradients to zero.\n",
    "            yhat = model.forward(data)  # get output\n",
    "            loss = criterion(yhat, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # statistic\n",
    "            lossTrain += loss.item() * len(label)\n",
    "            preds = yhat.max(1)[1]\n",
    "            predictions.extend(preds.int().tolist())\n",
    "            labels.extend(label.int().tolist())\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        lossTrain /= len(dTrain)\n",
    "        # train accuracy.\n",
    "        accTrain = accuracy_score(labels, predictions) * 100\n",
    "\n",
    "        # validation phase.\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for iter, (data, label) in enumerate(validloader):\n",
    "                # data conversion.\n",
    "                data = data.to(device)\n",
    "                label = label.contiguous().view(-1)\n",
    "                label = label.to(device)\n",
    "                # forward propagation.\n",
    "                yhat = model.forward(data)  # get output\n",
    "                # statistic\n",
    "                preds = yhat.max(1)[1]\n",
    "                predictions.extend(preds.int().tolist())\n",
    "                labels.extend(label.int().tolist())\n",
    "                torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # valid accuracy.\n",
    "        accValid = accuracy_score(labels, predictions) * 100\n",
    "        accList.append(accValid)\n",
    "\n",
    "        # testing phase.\n",
    "        if (markTest):\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for iter, (data, label) in enumerate(testloader):\n",
    "                    # data conversion.\n",
    "                    data = data.to(device)\n",
    "                    label = label.contiguous().view(-1)\n",
    "                    label = label.to(device)\n",
    "                    # forward propagation.\n",
    "                    yhat = model.forward(data)  # get output\n",
    "                    # statistic\n",
    "                    preds = yhat.max(1)[1]\n",
    "                    predictions.extend(preds.int().tolist())\n",
    "                    labels.extend(label.int().tolist())\n",
    "                    torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # test accuracy.\n",
    "            accTest = accuracy_score(labels, predictions) * 100\n",
    "\n",
    "        # output information.\n",
    "        if (0 == (epoch + 1) % _TRnnPerEpoch_):\n",
    "            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n",
    "            if (markTest):\n",
    "                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n",
    "            print(strAcc)\n",
    "        # save the best model.\n",
    "        if (accList[-1] > max(accList[0:-1])):\n",
    "            torch.save(model.state_dict(), tempPath + '/model_TwinRNN.pth')\n",
    "        # stop judgement.\n",
    "        if (epoch >= _TRnnJudEpoch_) and (accList[-1] < min(accList[-1-_TRnnJudEpoch_:-1])):\n",
    "            break\n",
    "\n",
    "    # load best model.\n",
    "    model.load_state_dict(torch.load(tempPath + '/model_TwinRNN.pth'))\n",
    "    print('[INFO] <TwinRNNTrain> Finish training TwinRNN model. (Best model: ' + tempPath + '/model_TwinRNN.pth)')\n",
    "\n",
    "    return model\n",
    "twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwinRNNTest(model, dTest, lTest, batchsize=64):\n",
    "    '''\n",
    "    Test the TwinRNN model.\n",
    "    :param model: deep learning model.\n",
    "    :param dTest: test data.\n",
    "    :param lTest: test label.\n",
    "    :param batchsize: number of samples in a batch\n",
    "    :return: predictions - predicted labels. [[0], [1], ...]\n",
    "             accuracy - the total test accuracy. numeric\n",
    "    '''\n",
    "\n",
    "    # tensor data processing.\n",
    "    xTest = torch.from_numpy(dTest).long().cuda()\n",
    "    yTest = torch.from_numpy(lTest).long().cuda()\n",
    "\n",
    "    # batch size processing.\n",
    "    test = torchdata.TensorDataset(xTest, yTest)\n",
    "    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "    # load the model of recurrent neural network.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # testing phase.\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for iter, (data, label) in enumerate(testloader):\n",
    "            # data conversion.\n",
    "            data = data.to(device)\n",
    "            label = label.contiguous().view(-1)\n",
    "            label = label.to(device)\n",
    "            # forward propagation.\n",
    "            yhat = model.forward(data)  # get output\n",
    "            # statistic\n",
    "            preds = yhat.max(1)[1]\n",
    "            predictions.extend(preds.int().tolist())\n",
    "            labels.extend(label.int().tolist())\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # testing accuracy.\n",
    "    accuracy = accuracy_score(labels, predictions) * 100\n",
    "    predictions = [[item] for item in predictions]\n",
    "\n",
    "    return predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1296840549.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    predictions, accuracy = TwinRNNTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " # TwinRNNTest\n",
    "   predictions, accuracy = TwinRNNTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n",
    "    _, confusion = OutputEval(predictions, labelTest, 'TwinRNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dvide before after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivideBeforeAfter(diffProps):\n",
    "    # create temp folder.\n",
    "    if not os.path.exists(tempPath):\n",
    "        os.mkdir(tempPath)\n",
    "    fp = open(tempPath + 'twinlen.csv', 'w')\n",
    "    \n",
    "    twinProps = []\n",
    "    maxLen = 0\n",
    "    # for each sample in diffProps.\n",
    "    for item in diffProps:\n",
    "        # get the tk, tkT, dfT, lb.\n",
    "        tokens = item[0]\n",
    "        tokenTypes = item[1]\n",
    "        diffTypes = item[2]\n",
    "        label = item[3]\n",
    "        numTokens = len(diffTypes)\n",
    "        # reconstruct tkB, tkTB, tkA, tkTA.\n",
    "        tokensB = [tokens[i] for i in range(numTokens) if (diffTypes[i] <= 0)]\n",
    "        tokenTypesB = [tokenTypes[i] for i in range(numTokens) if (diffTypes[i] <= 0)]\n",
    "        tokensA = [tokens[i] for i in range(numTokens) if (diffTypes[i] >= 0)]\n",
    "        tokenTypesA = [tokenTypes[i] for i in range(numTokens) if (diffTypes[i] >= 0)]\n",
    "        # reconstruct new sample.\n",
    "        sample = [tokensB, tokenTypesB, tokensA, tokenTypesA, label]\n",
    "        twinProps.append(sample)\n",
    "        # get max length.\n",
    "        maxLenAB = max(len(tokenTypesB), len(tokenTypesA))\n",
    "        maxLen = maxLenAB if (maxLen < maxLenAB) else maxLen\n",
    "        fp.write(str(len(tokenTypesB)) + '\\n')\n",
    "        fp.write(str(len(tokenTypesA)) + '\\n')\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
